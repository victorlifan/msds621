{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.datasets import load_boston, load_iris, load_wine, load_digits, \\\n",
    "                             load_breast_cancer, load_diabetes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from rfpimp import *\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "if LooseVersion(sklearn.__version__) >= LooseVersion(\"0.24\"):\n",
    "    # In sklearn version 0.24, forest module changed to be private.\n",
    "    from sklearn.ensemble._forest import _generate_unsampled_indices\n",
    "    from sklearn.ensemble import _forest as forest\n",
    "else:\n",
    "    # Before sklearn version 0.24, forest was public, supporting this.\n",
    "    from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "    from sklearn.ensemble import forest\n",
    "\n",
    "from sklearn import tree\n",
    "from dtreeviz.trees import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rent(n=None, bootstrap=False):\n",
    "    df_rent = pd.read_csv(\"data/rent-ideal.csv\")\n",
    "    if n is None:\n",
    "        n = len(df_rent)\n",
    "    df_rent = df_rent.sample(n, replace=bootstrap)\n",
    "    X = df_rent[['bedrooms','bathrooms','latitude','longitude']]\n",
    "    y = df_rent['price']\n",
    "    return X, y\n",
    "\n",
    "def boston():\n",
    "    boston = load_boston()\n",
    "    X = boston.data\n",
    "    y = boston.target\n",
    "    features = boston.feature_names\n",
    "    df = pd.DataFrame(data=X,columns=features)\n",
    "    df['y'] = y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "Get the `rent-ideal.csv`  data file from canvas and store in the data directory underneath your notebook  directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23112</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.7961</td>\n",
       "      <td>-73.9685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31274</th>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>40.7550</td>\n",
       "      <td>-73.9636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22091</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.7982</td>\n",
       "      <td>-73.9686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bedrooms  bathrooms  latitude  longitude\n",
       "23112         2        1.0   40.7961   -73.9685\n",
       "31274         2        1.5   40.7550   -73.9636\n",
       "22091         1        1.0   40.7982   -73.9686"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = rent()\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48300, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train random forests of different sizes\n",
    "\n",
    "As we increase the number of trees in the forest, we initially see model bias going down. It will asymptotically approach some minimum error on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to train a random forest  that has a single tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Compute the MAE for the training and the testing set, printing them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train = mean_absolute_error(...)\n",
    "mae = mean_absolute_error(...)\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Run the training and testing cycle several times to see the variance: the test scores bounce around a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Increase the number of trees (`n_estimators`) to 2, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=2)\n",
    "rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice the both test MAE scores going down and bouncing around less from run to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Why does the MAE score go down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    With 2 trees, the chances are that the random forest will have seen (trained on) more of the original training set, despite bootstrapping.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Increase the number of trees (`n_estimators`) to 10, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=10)\n",
    "rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  What you notice about the MAE scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "They are getting smaller.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  After running several times, what else do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    With 10 trees, the prediction from run to run varies a lot less. We have reduced variance,  improving generality.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Increase the number of trees (`n_estimators`) to 200, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=200)\n",
    "%time rf.fit(X_train, y_train) # how long does this take?\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  What you notice about the MAE scores from a single run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "They are a bit smaller, but not by much.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Notice that it took a long time to train, about 10 seconds.  Do the exact same thing again but this time use `n_jobs=-1` as an argument to the `RandomForestRegressor` constructor.\n",
    "\n",
    "This tells the library to use all processing cores available on the computer processor. As long as the data is not too huge (because it must pass it around), it often goes much faster using this argument. It should take less than two seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  What you notice about the MAE scores from SEVERAL runs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The error variance across runs is even lower (tighter).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining model size and complexity\n",
    "\n",
    "The structure of a tree is affected by a number of hyper parameters, not just the data. Goal in the section is to see the effect of altering the number of samples per leaf and the maximum number of candidate features per split. Let's start out with a handy function that uses some  support code from rfpimp to examine tree size and depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showsize(ntrees, max_features=1.0, min_samples_leaf=1):\n",
    "    rf = RandomForestRegressor(n_estimators=ntrees,\n",
    "                               max_features=max_features,\n",
    "                               min_samples_leaf=min_samples_leaf,\n",
    "                               n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    n = rfnnodes(rf)                # from rfpimp\n",
    "    h = np.median(rfmaxdepths(rf))  # rfmaxdepths from rfpimp\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:6.1f}$, test {mae:6.1f}$ using {n:9,d} tree nodes with {h:2.0f} median tree height\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of number of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single tree, we see about 21,000 nodes and a tree height of around 35:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showsize(ntrees=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Look at the metrics for 2 trees and then 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "showsize(ntrees=2)\n",
    "showsize(ntrees=100)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why does the median height of a tree stay the same when we increase the number of trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "While the number of nodes increases with the number of trees, the height of any individual tree will stay the same because we have not fundamentally changed how it is constructing a single tree.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of increasing min samples / leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Loop around a call to `showsize()` with 10 trees and min_samples_leaf=1..10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(...):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "for i in range(1,10+1):\n",
    "    showsize(ntrees=10, min_samples_leaf=i)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why do the median height of a tree and number of total nodes decrease as we increase the number of samples per leaf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Because  when the sample size gets down to `min_samples_leaf`, splitting stops, which prevents the tree from getting taller. It also restricts how many nodes total get created for the tree.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Why does the MAE error increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "If we include more observations in a single leaf, then the average is taken over more samples. That average is a more general prediction but less accurate.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty clear from that print out that `min_samples_leaf=1` is the best choice because it gives the minimum validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of reducing max_features (rent data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Do another loop from `max_features` = 4 down to 1, with 1 sample per leaf. (There are 4 total features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X_train.shape[1]\n",
    "for i in range(...):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(ntrees=10, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "p = X_train.shape[1]\n",
    "for i in range(p,0,-1):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(ntrees=10, max_features=i)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data set,  changing the available candidate features that each split does not seem to be important as the validation error does not change, nor does the height of the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine effects of hyper parameters on Boston data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston = boston()\n",
    "df_boston.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_boston.drop('y', axis=1), df_boston['y']\n",
    "y *= 1000 # y is \"Median value of owner-occupied homes in $1000's\" so multiply by 1000\n",
    "\n",
    "# reproducible 20% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the metric `showsize()` function to see how many trees we should use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,5,30,50,100,150,300]:\n",
    "    print(f\"{i:3d} trees: \", end='')\n",
    "    showsize(ntrees=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the sweet spot on the validation error is probably 150 trees as it gets a low validation error and has a fairly small set of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the effect of increasing the minimum samples per leaf from 1 to 10 as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10+1):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(ntrees=150, min_samples_leaf=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error goes up dramatically but the validation error doesn't get too much worse.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Which min samples per leaf would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    After running a few times, it seems that using <tt>min_samples_leaf</tt>=1 or 2 is best for the validation error. But, keep in mind that this data set is pretty small and so our error values will change quite a bit depending on the sample we get for the test set.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a loop from the maximum number of features down to 1 for `max_features` to see the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X_train.shape[1]\n",
    "for i in range(p,0,-1):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(ntrees=150, max_features=i, min_samples_leaf=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Which max features would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    After running a few times, it seems that using <tt>max_features</tt>=7 or 13 gets best validation error, but again it depends on the randomness of the tree construction and results will vary across runs.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the final model would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showsize(ntrees=150, max_features=13, min_samples_leaf=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF prediction confidence\n",
    "\n",
    "A random forest is a collection of decision trees, each of which contributes a prediction. The forest averages those predictions to provide the overall prediction (or takes most common vote for classification). Let's dig inside the random forest to get the individual trees out and ask them what their predictions are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Train a random forest with 10 trees on `X_train`, `y_train`.  Use `for t in rf.estimators_` to iterate through the trees making predictions with `t` not `rf`. Print out the usual MAE scores for each tree predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "for t in ...:\n",
    "    mae_train = ...\n",
    "    mae = ...\n",
    "    print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "for t in rf.estimators_:\n",
    "    mae_train = mean_absolute_error(y_train, t.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, t.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it bounces around quite a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Select one of the `X_test` rows and print out the addicted rent price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ... # pick single test case\n",
    "x = x.values.reshape(1,-1) # Needs to be a one-row matrix\n",
    "\n",
    "print(f\"{x} => {rf.predict(x)}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "x = X_test.iloc[3,:] # pick single test case\n",
    "x = x.values.reshape(1,-1)\n",
    "print(f\"{x} => {rf.predict(x)}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Now let's see how the forest came to that conclusion. Compute the average of the predictions obtained from every tree.  \n",
    "\n",
    "Compare that to the prediction obtained directly from the random forest (`rf.predict(X_test)`). They should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ...\n",
    "print(f\"{x} => {y_pred}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "y_pred = np.mean([t.predict(x) for t in rf.estimators_])\n",
    "print(f\"{x} => {y_pred}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Compute the standard deviation of the tree estimates and print that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "np.std([t.predict(x) for t in rf.estimators_])\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the standard deviation, the more tightly grouped the predictions were, which means we should have more confidence in our answer. \n",
    "\n",
    "Different records will often have different standard deviations, which means we could have different levels of confidence in the various answers. This might be helpful to a bank for example that wanted to not only predict whether to give loans, but how confident the model was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering bootstrap size\n",
    "\n",
    "**This no longer works with latest versions of scikit-learn... and the feature is not yet implemented by them* See [related github issue](https://github.com/scikit-learn/scikit-learn/issues/11993).  Ah [this new features](https://github.com/scikit-learn/scikit-learn/pull/14682) covers it for trees. \"Adds a max_samples kwarg to forest ensembles that limits the size of the bootstrap samples used to train each estimator.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NO LONGER NEEDED\n",
    "def jeremy_trick_RF_sample_size(n):\n",
    "    if LooseVersion(sklearn.__version__) >= LooseVersion(\"0.24\"):\n",
    "        forest._generate_sample_indices = \\\n",
    "            (lambda rs, n_samples, _:\n",
    "             forest.check_random_state(rs).randint(0, n_samples, n))\n",
    "    else:\n",
    "        forest._generate_sample_indices = \\\n",
    "            (lambda rs, n_samples: forest.check_random_state(rs).randint(0, n_samples, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = rent()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: There are about 38,000 training records, change that to 19,000 and check the accuracy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.64 s, sys: 235 ms, total: 8.88 s\n",
      "Wall time: 8.95 s\n",
      "MAE train 184.2$, test 291.1$\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200) # don't compute in parallel so we can see timing\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.75 s, sys: 122 ms, total: 5.88 s\n",
      "Wall time: 5.96 s\n",
      "MAE train 226.7$, test 307.1$\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, max_samples=1/2)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit less accurate, but it's faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Why is it less accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Each tree is seeing less of the data set during training.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Turn off bootstrapping by adding `bootstrap=False` to the constructor of the model. This means that it will subsample rather than bootstrap. Remember that bootstrapping gets about two thirds of the data because of replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1, bootstrap=False)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That brings the accuracy back up a little bit for the test set but very much so for the training MAE score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Drop that size to one third of the training records then retrain and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeremy_trick_RF_sample_size(round(len(X_train)/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine is twice as fast as the full bootstrap but continues to have very tight variance because of the number of trees. The accuracy is lower, however, about what we get for the  usual random forest with two trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
